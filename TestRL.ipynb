{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestRL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdpZ5mk08mGiTu0rI4OFIH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/decomiteA/ReachRLToolbox/blob/main/TestRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMaxRdwPYscQ"
      },
      "source": [
        "This notebook models reaching movement using reinforcement learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "uj7Xf9xyYrMF"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from scipy.signal import convolve as conv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GSphTpHhZDtE"
      },
      "source": [
        "#@title Plot handling\n",
        "\n",
        "def plot_state_action_values(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing value of each action at each state.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  for a in range(env.n_actions):\n",
        "    ax.plot(range(env.n_states), value[:, a], marker='o', linestyle='--')\n",
        "  ax.set(xlabel='States', ylabel='Values')\n",
        "  ax.legend(['R','U','L','D'], loc='lower right')\n",
        "\n",
        "def plot_quiver_max_action(env,value,ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing action of maximum value or maximum probability for each state\n",
        "  \"\"\"\n",
        "  if ax is None: \n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  X = np.tile(np.arange(env.dim_x),[env.dim_y,1])+0.5\n",
        "  Y = np.tile(np.arange(env.dim_y)[::-1][:,np.newaxis],[1,env.dim_x])\n",
        "  which_max = np.reshape(value.argmax(axis=1),(env.dim_y,env.dim_x))\n",
        "  which_max = which_max[::-1,:]\n",
        "  U = np.zeros(X.shape)\n",
        "  V = np.zeros(Y.shape)\n",
        "  U[which_max == 0] = 1\n",
        "  V[which_max == 1] = 1\n",
        "  U[which_max == 2] = -1\n",
        "  V[which_max == 3] = -1\n",
        "\n",
        "  ax.quiver(X,Y,U,V)\n",
        "  ax.set(\n",
        "      title='Maximum value/probability actions',\n",
        "      xlim = [-0.5, env.dim_x+0.5],\n",
        "      ylim = [-0.5, env.dim_y+0.5],\n",
        "  )\n",
        "  ax.set_xticks(np.linspace(0.5, env.dim_x-0.5, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_xticks(np.arange(env.dim_x+1), minor=True)\n",
        "  ax.set_yticks(np.linspace(0.5, env.dim_y-0.5, num=env.dim_y))\n",
        "  ax.set_yticklabels([\"%d\" % y for y in np.arange(0, env.dim_y*env.dim_x,\n",
        "                                                  env.dim_x)])\n",
        "  ax.set_yticks(np.arange(env.dim_y+1), minor=True)\n",
        "  ax.grid(which='minor',linestyle='-')\n",
        "\n",
        "  \n",
        "def plot_heatmap_max_val(env, value, ax=None):\n",
        "  \"\"\"\n",
        "  Generate heatmap showing maximum value at each state\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  if value.ndim == 1:\n",
        "      value_max = np.reshape(value, (env.dim_y,env.dim_x))\n",
        "  else:\n",
        "      value_max = np.reshape(value.max(axis=1), (env.dim_y,env.dim_x))\n",
        "  value_max = value_max[::-1,:]\n",
        "\n",
        "  im = ax.imshow(value_max, aspect='auto', interpolation='none', cmap='afmhot')\n",
        "  ax.set(title='Maximum value per state')\n",
        "  ax.set_xticks(np.linspace(0, env.dim_x-1, num=env.dim_x))\n",
        "  ax.set_xticklabels([\"%d\" % x for x in np.arange(env.dim_x)])\n",
        "  ax.set_yticks(np.linspace(0, env.dim_y-1, num=env.dim_y))\n",
        "  if env.name != 'windy_cliff_grid':\n",
        "      ax.set_yticklabels(\n",
        "          [\"%d\" % y for y in np.arange(\n",
        "              0, env.dim_y*env.dim_x, env.dim_x)][::-1])\n",
        "  return im\n",
        "\n",
        "\n",
        "def plot_rewards(n_episodes, rewards, average_range=10, ax=None):\n",
        "  \"\"\"\n",
        "  Generate plot showing total reward accumulated in each episode.\n",
        "  \"\"\"\n",
        "  if ax is None:\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "  smoothed_rewards = (conv(rewards, np.ones(average_range), mode='same')\n",
        "                      / average_range)\n",
        "\n",
        "  ax.plot(range(0, n_episodes, average_range),\n",
        "          smoothed_rewards[0:n_episodes:average_range],\n",
        "          marker='o', linestyle='--')\n",
        "  ax.set(xlabel='Episodes', ylabel='Total reward')\n",
        "\n",
        "\n",
        "def plot_performance(env, value, reward_sums):\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
        "  plot_state_action_values(env, value, ax=axes[0,0])\n",
        "  plot_quiver_max_action(env, value, ax=axes[0,1])\n",
        "  plot_rewards(n_episodes, reward_sums, ax=axes[1,0])\n",
        "  im = plot_heatmap_max_val(env, value, ax=axes[1,1])\n",
        "  fig.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA-Rf3pkZQQy"
      },
      "source": [
        "The section of code below defines the different environment as objects with their own functions. Each environment is explained in the comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "Rm_QoyToZubl"
      },
      "source": [
        "#@title Environments definition\n",
        "\n",
        "class ToyExample:\n",
        "  \"\"\"\n",
        "  This class defines the toy example that consists of a 4x4 grid with one good target state\n",
        "  Here are the different states : \n",
        "  12 13 14 15\n",
        "  8  9  10 11\n",
        "  4  5  6  7\n",
        "  1  2  3  4\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.name=\"ToyExample1\"\n",
        "    self.n_states=16\n",
        "    self.n_actions=4\n",
        "    self.dim_x=4\n",
        "    self.dim_y=5\n",
        "    self.init_state=np.random.choice(range(n_states))\n",
        "\n",
        "  def get_outcome(self,state,action):\n",
        "    \"\"\"\n",
        "    This function retunrs the outcome of taking an action in a given state (returns reward and next_state)\n",
        "    \"\"\"\n",
        "    if state==14: \n",
        "      reward = 0\n",
        "      next_state = None\n",
        "      return next_state,reward \n",
        "\n",
        "    reward = -1 #we penalise any other action\n",
        "    if action==0:   #move right\n",
        "      if state%4==3: #right border \n",
        "        next_state=state\n",
        "      else:\n",
        "        next_state=state+1\n",
        "      \n",
        "    elif action==1: #move up\n",
        "      if state>11:\n",
        "        next_state=state\n",
        "      else:\n",
        "        next_state=state+4\n",
        "      \n",
        "    elif action==2: #move left\n",
        "      if state%4==0: \n",
        "        next_state=state\n",
        "      else:\n",
        "        next_state=state-1\n",
        "    \n",
        "    elif action==3: #move down\n",
        "      if state<4:\n",
        "        next_state=state\n",
        "      else:\n",
        "        next_state=state-4\n",
        "      \n",
        "    else:\n",
        "      print(\"Incorrect action, the selected action should be between 0 and 3\")\n",
        "      next_state=None\n",
        "      reward=None\n",
        "    return int(next_state) if next_state is not None else None, reward\n",
        "\n",
        "  def get_all_outcomes(self):\n",
        "    outcomes={}\n",
        "    for state in range(self.n_states):\n",
        "      for action in range(self.n_actions):\n",
        "        next_state,reward = self.get_outcome(state,action)\n",
        "        outcomes[state,action]=[(1,next_state,reward)]\n",
        "    return outcomes\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}