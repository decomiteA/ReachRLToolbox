{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fdffcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ff_environment import ForceField\n",
    "from agent import Agent\n",
    "from trajectories import Trajectories\n",
    "from collections import deque\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050fc1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ForceField(space_padding=2)\n",
    "env_info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3973ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 2\n",
      "The agent observes a state with length: 4\n",
      "The starting state looks like: [0.5 1.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "# size of each action\n",
    "action_size = env.action_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.state\n",
    "state_size = len(state)\n",
    "print('The agent observes a state with length: {}'.format(state_size))\n",
    "print('The starting state looks like:', state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3661a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent:\n",
    "agent = Agent(state_size, action_size, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 0 \tAverage Reward: -6.28\r",
      "Episode 0 \tAverage Reward: -6.28\n",
      "\r",
      "Episode 1 \tAverage Reward: -6.02\r",
      "Episode 2 \tAverage Reward: -5.97\r",
      "Episode 3 \tAverage Reward: -5.97\r",
      "Episode 4 \tAverage Reward: -4.02\r",
      "Episode 5 \tAverage Reward: -4.47\r",
      "Episode 6 \tAverage Reward: -4.62\r",
      "Episode 7 \tAverage Reward: -4.76\r",
      "Episode 8 \tAverage Reward: -4.83\r",
      "Episode 9 \tAverage Reward: -4.92\r",
      "Episode 10 \tAverage Reward: -4.96\r",
      "Episode 11 \tAverage Reward: -5.01\r",
      "Episode 12 \tAverage Reward: -5.05\r",
      "Episode 13 \tAverage Reward: -5.13\r",
      "Episode 14 \tAverage Reward: -5.14\r",
      "Episode 15 \tAverage Reward: -5.15\r",
      "Episode 16 \tAverage Reward: -5.16\r",
      "Episode 17 \tAverage Reward: -5.17\r",
      "Episode 18 \tAverage Reward: -5.19\r",
      "Episode 19 \tAverage Reward: -5.21\r",
      "Episode 20 \tAverage Reward: -5.23\r",
      "Episode 21 \tAverage Reward: -5.24\r",
      "Episode 22 \tAverage Reward: -5.25\r",
      "Episode 23 \tAverage Reward: -5.26\r",
      "Episode 24 \tAverage Reward: -5.27\r",
      "Episode 25 \tAverage Reward: -5.28\r",
      "Episode 26 \tAverage Reward: -5.28\r",
      "Episode 27 \tAverage Reward: -5.28\r",
      "Episode 28 \tAverage Reward: -5.28\r",
      "Episode 29 \tAverage Reward: -5.29\r",
      "Episode 30 \tAverage Reward: -4.96\r",
      "Episode 31 \tAverage Reward: -4.65\r",
      "Episode 32 \tAverage Reward: -4.37\r",
      "Episode 33 \tAverage Reward: -4.09\r",
      "Episode 34 \tAverage Reward: -3.84\r",
      "Episode 35 \tAverage Reward: -3.88"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 \tAverage Reward: -4.65\n",
      "Episode 200 \tAverage Reward: -5.56\n",
      "Episode 300 \tAverage Reward: -5.50\n",
      "Episode 334 \tAverage Reward: -5.45"
     ]
    }
   ],
   "source": [
    "# train the agent with ddpg\n",
    "scores, trajectories, actions_tracker = agent.train_ddpg(env, n_episodes = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f5ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories.plot(3, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab172c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
