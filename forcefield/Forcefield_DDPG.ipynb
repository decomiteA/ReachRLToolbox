{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93393238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ff_environment import ForceField\n",
    "from agent import Agent\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e2df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ForceField()\n",
    "env_info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977c4c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 2\n",
      "The agent observes a state with length: 4\n",
      "The starting state looks like: [0.5 1.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "# size of each action\n",
    "action_size = env.action_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.state\n",
    "state_size = len(state)\n",
    "print('The agent observes a state with length: {}'.format(state_size))\n",
    "print('The starting state looks like:', state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0cafb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent:\n",
    "agent = Agent(state_size, action_size, random_seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba0040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 0 \tAverage Reward: -1.69\r",
      "Episode 0 \tAverage Reward: -1.69\n",
      "\r",
      "Episode 1 \tAverage Reward: -4.14\r",
      "Episode 2 \tAverage Reward: -4.36\r",
      "Episode 3 \tAverage Reward: -3.99\r",
      "Episode 4 \tAverage Reward: -3.52\r",
      "Episode 5 \tAverage Reward: -3.20\r",
      "Episode 6 \tAverage Reward: -2.92\r",
      "Episode 7 \tAverage Reward: -2.71\r",
      "Episode 8 \tAverage Reward: -2.55\r",
      "Episode 9 \tAverage Reward: -2.41"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 \tAverage Reward: -1.70\n",
      "Episode 2000 \tAverage Reward: -2.48\n",
      "Episode 3000 \tAverage Reward: -1.67\n",
      "Episode 4000 \tAverage Reward: -0.90\n",
      "Episode 4999 \tAverage Reward: -3.23"
     ]
    }
   ],
   "source": [
    "# train the agent with ddpg\n",
    "def ddpg(n_episodes=5000, max_t=1000, print_every=1000):\n",
    "\n",
    "    scores = []\n",
    "    trajectories = [] \n",
    "    actions_tracker = []\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    \n",
    "    for i_episode in range(n_episodes):\n",
    "        env_info = env.reset()\n",
    "        state = env_info.state        # current state\n",
    "        score = 0                      # initialize agent scores\n",
    "        trajectory = [state[:2]]           # initialize trajectory \n",
    "        actions = [state[2:]]\n",
    "        agent.reset()                  # reset noise process for action exploration\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            \n",
    "            env_info = env.step(action)               # send action to environment\n",
    "            next_state = env_info.state               # get next state \n",
    "            reward = env_info.reward                  # get reward \n",
    "            done = env_info.done                      # see if trial is finished\n",
    "            \n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            score += reward                         # update the score (for each agent)\n",
    "            state = next_state                               # enter next states\n",
    "            trajectory.append(env_info.pos)\n",
    "            actions.append(action)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_deque.append(np.mean(score))\n",
    "        scores.append(np.mean(score))\n",
    "        trajectories.append(trajectory)\n",
    "        actions_tracker.append(actions)\n",
    "        \n",
    "        print('\\rEpisode {} \\tAverage Reward: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_model.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_model.pth')\n",
    "            print('\\rEpisode {} \\tAverage Reward: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "    \n",
    "        if np.mean(scores_deque) >= 0.07:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\t Average Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_solved.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'critic_solved.pth')\n",
    "            break\n",
    "            \n",
    "    return scores, trajectories, actions_tracker\n",
    "\n",
    "scores, trajectories, actions_tracker = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21202d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.5, 1. ]),\n",
       " (0.42416413128376007, 0.8014600425958633),\n",
       " (0.2995334193110466, 0.49080153554677963),\n",
       " (0.1393531896173954, 0.11065014824271202),\n",
       " (-0.04938287101686001, -0.3173066843301058),\n",
       " (-0.2623219592496753, -0.7812156463041902),\n",
       " (-0.4961089021526277, -1.2737566572614014),\n",
       " (-0.7477115679066628, -1.7896811387035996),\n",
       " (-1.0142132559558377, -2.324729488347657),\n",
       " (-1.292814779735636, -2.875206359254662),\n",
       " (-1.5809119211917277, -3.437854487710865),\n",
       " (-1.876179263854283, -4.0098453109530965),\n",
       " (-2.176629543733725, -4.588797760261514),\n",
       " (-2.480638630902831, -5.172784647471417),\n",
       " (-2.786939021307262, -5.760313744905943),\n",
       " (-3.094589090028421, -6.3502847679274055),\n",
       " (-3.4029277516797265, -6.941930345089077),\n",
       " (-3.7115230313636403, -7.534750293389379),\n",
       " (-4.0201210330773165, -8.128447552867442),\n",
       " (-4.328599696400204, -8.722871328566328),\n",
       " (-4.636929924310465, -9.3179700946678),\n",
       " (-4.945144285768876, -9.91375516235179),\n",
       " (-5.253313141569798, -10.510274059471222),\n",
       " (-5.561527119331213, -11.107591677157448),\n",
       " (-5.86988504293196, -11.70577819071629),\n",
       " (-6.17848552794572, -12.30490132134367),\n",
       " (-6.487421825683695, -12.905021990146285),\n",
       " (-6.796778768378785, -13.506192025942626),\n",
       " (-7.106631286481168, -14.10845376422158),\n",
       " (-7.417043831218852, -14.711839846621707),\n",
       " (-7.728070488209314, -15.31637395513592),\n",
       " (-8.039755570707117, -15.922071626248647),\n",
       " (-8.352134288756746, -16.528941134841585),\n",
       " (-8.665233635111997, -17.13698435337908),\n",
       " (-8.979073349736309, -17.74619771794263),\n",
       " (-9.293666595768023, -18.356572711248116),\n",
       " (-9.6090208041425, -18.968096640596247),\n",
       " (-9.925138393926394, -19.580753116254733),\n",
       " (-10.24201749011267, -20.1945225886726),\n",
       " (-10.55965238682622, -20.80938270649431),\n",
       " (-10.878034091851088, -21.425308584973802),\n",
       " (-11.19715076280682, -22.0422730891931),\n",
       " (-11.516988059347117, -22.66024706507861),\n",
       " (-11.837529632183534, -23.279199752932485),\n",
       " (-12.158757218585722, -23.899098696175066),\n",
       " (-12.480651003055927, -24.519910023541673),\n",
       " (-12.803189887073218, -25.141598709390273),\n",
       " (-13.126351638866753, -25.76412858464604),\n",
       " (-13.45011314711635, -26.387462640296842),\n",
       " (-13.774450518000448, -27.011563030129384),\n",
       " (-14.099339287632866, -27.636391340318177)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977238b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
